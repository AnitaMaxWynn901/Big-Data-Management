{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "invalid-saudi",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.021939,
     "end_time": "2022-07-11T09:42:10.370150",
     "exception": false,
     "start_time": "2022-07-11T09:42:10.348211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Market Basket Analysis using PySpark's Implementation of FPGrowth\n",
    "\n",
    "FPGrowth is an algorithm that performs market basket analysis, similar to the Apriori algorithm. I first used it when I ran into resource issues with Apriori and I was impressed with the speed. So I am giving it a try on this dataset using pyspark. The [documentation for FPGrowth](https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html) is pretty straightforward and describes the hyperparameters and the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-sector",
   "metadata": {
    "papermill": {
     "duration": 0.177692,
     "end_time": "2022-07-11T09:43:11.595889",
     "exception": false,
     "start_time": "2022-07-11T09:43:11.418197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import the relevant libraries\n",
    "\n",
    "The libraries such as SparkContext and SparkSession are general pyspark libraries needed for pyspark applications. The specific function used for market basket analysis is [FPGrowth](https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df3752c-ab31-4e3a-a269-82b0cc47feab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark_dist_explore in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.1.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyspark_dist_explore) (2.2.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyspark_dist_explore) (2.2.2)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyspark_dist_explore) (1.15.1)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyspark_dist_explore) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->pyspark_dist_explore) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->pyspark_dist_explore) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->pyspark_dist_explore) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->pyspark_dist_explore) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->pyspark_dist_explore) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->pyspark_dist_explore) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->pyspark_dist_explore) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib->pyspark_dist_explore) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->pyspark_dist_explore) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->pyspark_dist_explore) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->pyspark_dist_explore) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " # Used for a histogram\n",
    "!pip install pyspark_dist_explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extensive-simon",
   "metadata": {
    "papermill": {
     "duration": 0.96355,
     "end_time": "2022-07-11T09:43:12.734780",
     "exception": false,
     "start_time": "2022-07-11T09:43:11.771230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# Rather than generally using the functions, I should explicitly import the ones I want.\n",
    "from pyspark.sql import functions as f, SparkSession, Column\n",
    "from pyspark_dist_explore import hist\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.fpm import FPGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "driving-shirt",
   "metadata": {
    "papermill": {
     "duration": 7.439415,
     "end_time": "2022-07-11T09:43:20.692621",
     "exception": false,
     "start_time": "2022-07-11T09:43:13.253206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 17:19:35 WARN Utils: Your hostname, Jennie-Kims-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.137 instead (on interface en0)\n",
      "25/03/16 17:19:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/16 17:19:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session. All sorts of settings can be specified here. \n",
    "spark = SparkSession.builder.appName(\"arlUsingPyspark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unique-dover",
   "metadata": {
    "papermill": {
     "duration": 8.0415,
     "end_time": "2022-07-11T09:43:29.247707",
     "exception": false,
     "start_time": "2022-07-11T09:43:21.206207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = spark.read.csv(\"basket.csv\", header=True).withColumn(\"id\", f.monotonically_increasing_id())\n",
    "#df_all = spark.read.csv(\"/Users/admin/Jupyter Examples/Groceries data.csv\", header=True).withColumn(\"id\", f.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "junior-chicago",
   "metadata": {
    "papermill": {
     "duration": 1.041381,
     "end_time": "2022-07-11T09:43:30.467483",
     "exception": false,
     "start_time": "2022-07-11T09:43:29.426102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-------------------+------+----+----+----+----+----+----+----+---+\n",
      "|          0|                 1|                  2|     3|   4|   5|   6|   7|   8|   9|  10| id|\n",
      "+-----------+------------------+-------------------+------+----+----+----+----+----+----+----+---+\n",
      "| whole milk|            pastry|        salty snack|  NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|  0|\n",
      "|    sausage|        whole milk|semi-finished bread|yogurt|NULL|NULL|NULL|NULL|NULL|NULL|NULL|  1|\n",
      "|       soda|pickled vegetables|               NULL|  NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|  2|\n",
      "|canned beer|   misc. beverages|               NULL|  NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|  3|\n",
      "|    sausage|  hygiene articles|               NULL|  NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|  4|\n",
      "+-----------+------------------+-------------------+------+----+----+----+----+----+----+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the dataframes\n",
    "df.show(5)\n",
    "#df_all.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "challenging-sandwich",
   "metadata": {
    "papermill": {
     "duration": 2.108782,
     "end_time": "2022-07-11T09:43:33.883989",
     "exception": false,
     "start_time": "2022-07-11T09:43:31.775207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#num_baskets = df_all.groupBy(\"Member_number\").count()\n",
    "#num_baskets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "embedded-reply",
   "metadata": {
    "papermill": {
     "duration": 2.51904,
     "end_time": "2022-07-11T09:43:37.053373",
     "exception": false,
     "start_time": "2022-07-11T09:43:34.534333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots()\n",
    "#hist(ax, num_baskets.select('count'), bins = 30, color=['blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-neighborhood",
   "metadata": {
    "papermill": {
     "duration": 0.176517,
     "end_time": "2022-07-11T09:43:37.424663",
     "exception": false,
     "start_time": "2022-07-11T09:43:37.248146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run PySpark's implementation of FPGrowth\n",
    "\n",
    "First step is to collect the baskets into sets. FPGrowth requires each basket to be an array that looks like:\n",
    "\n",
    "* ['item1','item2', 'imem3']\n",
    "\n",
    "The basket dataframe uses wide rather than long format, with Null if the basket contains fewer than 10 items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "celtic-uganda",
   "metadata": {
    "papermill": {
     "duration": 0.465601,
     "end_time": "2022-07-11T09:43:38.059790",
     "exception": false,
     "start_time": "2022-07-11T09:43:37.594189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------------------------+\n",
      "|id |basket                                                                                      |\n",
      "+---+--------------------------------------------------------------------------------------------+\n",
      "|0  |[whole milk, pastry, salty snack, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL]           |\n",
      "|1  |[sausage, whole milk, semi-finished bread, yogurt, NULL, NULL, NULL, NULL, NULL, NULL, NULL]|\n",
      "|2  |[soda, pickled vegetables, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL]            |\n",
      "+---+--------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_basket = df.select(\"id\", f.array([df[c] for c in df.columns[:11]]).alias(\"basket\"))\n",
    "# False tells show() to not truncate the columns when printing.\n",
    "df_basket.show(3, False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-importance",
   "metadata": {
    "papermill": {
     "duration": 0.17694,
     "end_time": "2022-07-11T09:43:38.479650",
     "exception": false,
     "start_time": "2022-07-11T09:43:38.302710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### There should not be any nulls in the array. Remove using array_except()\n",
    "\n",
    "This will be the final dataframe used for FPGrowth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "under-reminder",
   "metadata": {
    "papermill": {
     "duration": 0.521651,
     "end_time": "2022-07-11T09:43:39.220703",
     "exception": false,
     "start_time": "2022-07-11T09:43:38.699052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------+\n",
      "|id |basket                                            |\n",
      "+---+--------------------------------------------------+\n",
      "|0  |[whole milk, pastry, salty snack]                 |\n",
      "|1  |[sausage, whole milk, semi-finished bread, yogurt]|\n",
      "|2  |[soda, pickled vegetables]                        |\n",
      "+---+--------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aggregated = df_basket.select(\"id\", f.array_except(\"basket\", f.array(f.lit(None))).alias(\"basket\"))\n",
    "df_aggregated.show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-antique",
   "metadata": {
    "papermill": {
     "duration": 0.167719,
     "end_time": "2022-07-11T09:43:39.585152",
     "exception": false,
     "start_time": "2022-07-11T09:43:39.417433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "The hyperparameters used in FPGrowth are minimum support, minimum confidence, and number of partitions. \n",
    "\n",
    "* minSupport - The minimum support of an item to be considered in a frequent itemset. \n",
    "* minConfidence - The minimum confidence for generating an association rule from an itemset. \n",
    "* numPartitions - The number of partitions used to distribute the work. This is Spark-specific. \n",
    "\n",
    "The default number of partitions is the number of partitions for the input dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "floating-stress",
   "metadata": {
    "papermill": {
     "duration": 1.600979,
     "end_time": "2022-07-11T09:43:41.353520",
     "exception": false,
     "start_time": "2022-07-11T09:43:39.752541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run FPGrowth and fit the model.\n",
    "fp = FPGrowth(minSupport=0.001, minConfidence=0.001, itemsCol='basket', predictionCol='prediction')\n",
    "model = fp.fit(df_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "curious-summer",
   "metadata": {
    "papermill": {
     "duration": 1.735533,
     "end_time": "2022-07-11T09:43:43.268545",
     "exception": false,
     "start_time": "2022-07-11T09:43:41.533012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----+\n",
      "|items                   |freq|\n",
      "+------------------------+----+\n",
      "|[cocoa drinks]          |16  |\n",
      "|[canned fruit]          |21  |\n",
      "|[specialty cheese]      |72  |\n",
      "|[chocolate marshmallow] |60  |\n",
      "|[pet care]              |85  |\n",
      "|[house keeping products]|45  |\n",
      "|[jam]                   |34  |\n",
      "|[light bulbs]           |29  |\n",
      "|[beef]                  |508 |\n",
      "|[beef, frankfurter]     |15  |\n",
      "+------------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View a subset of the frequent itemset. \n",
    "model.freqItemsets.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "closing-organization",
   "metadata": {
    "papermill": {
     "duration": 1.349187,
     "end_time": "2022-07-11T09:43:44.890107",
     "exception": false,
     "start_time": "2022-07-11T09:43:43.540920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------+-------------------+------------------+---------------------+\n",
      "|antecedent           |consequent  |confidence         |lift              |support              |\n",
      "+---------------------+------------+-------------------+------------------+---------------------+\n",
      "|[bottled beer]       |[whole milk]|0.15781710914454278|0.9993302598941151|0.007150972398583172 |\n",
      "|[detergent]          |[whole milk]|0.16279069767441862|1.030824041177455 |0.001403461872619127 |\n",
      "|[semi-finished bread]|[whole milk]|0.176056338028169  |1.1148247930239072|0.001670787943594199 |\n",
      "|[sausage, rolls/buns]|[whole milk]|0.2125             |1.345593525179856 |0.0011361358016440553|\n",
      "|[sausage, soda]      |[whole milk]|0.1797752808988764 |1.1383739010113787|0.0010693042839002875|\n",
      "|[ham]                |[whole milk]|0.16015625         |1.0141421789039358|0.0027400922274944863|\n",
      "|[frozen fish]        |[whole milk]|0.1568627450980392 |0.9932870312746344|0.0010693042839002875|\n",
      "|[sausage, whole milk]|[yogurt]    |0.16417910447761194|1.9117602648237413|0.0014702933903628951|\n",
      "|[sausage, yogurt]    |[whole milk]|0.2558139534883721 |1.6198663504217148|0.0014702933903628951|\n",
      "|[yogurt, rolls/buns] |[whole milk]|0.17094017094017094|1.0824281751069733|0.0013366303548753592|\n",
      "+---------------------+------------+-------------------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use filter to view just the association rules with the highest confidence.\n",
    "model.associationRules.filter(model.associationRules.confidence>0.15).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-darwin",
   "metadata": {
    "papermill": {
     "duration": 0.173733,
     "end_time": "2022-07-11T09:43:45.249247",
     "exception": false,
     "start_time": "2022-07-11T09:43:45.075514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Let's create a prediction based on the generated association rules\n",
    "\n",
    "This is pretty similar to creating a prediction using other methods. The data column needs to have the same column name as the column specified in the model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "excessive-horizon",
   "metadata": {
    "papermill": {
     "duration": 0.702297,
     "end_time": "2022-07-11T09:43:46.130513",
     "exception": false,
     "start_time": "2022-07-11T09:43:45.428216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|basket                       |\n",
      "+-----------------------------+\n",
      "|[ham, yogurt, light bulbs]   |\n",
      "|[jam, cocoa drinks, pet care]|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a PySpark dataframe\n",
    "columns = ['basket']\n",
    "new_data = [(['ham', 'yogurt', 'light bulbs'],), (['jam', 'cocoa drinks', 'pet care'],)]\n",
    "rdd = spark.sparkContext.parallelize(new_data)\n",
    "new_df = rdd.toDF(columns)\n",
    "new_df.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-treaty",
   "metadata": {
    "papermill": {
     "duration": 0.170707,
     "end_time": "2022-07-11T09:43:46.476281",
     "exception": false,
     "start_time": "2022-07-11T09:43:46.305574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict!\n",
    "\n",
    "Now that we have a new PySpark dataframe with data, predict. The first basket generates numerous predictions based on the association rules, however the second basket does not generate any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "specialized-medicine",
   "metadata": {
    "papermill": {
     "duration": 0.863093,
     "end_time": "2022-07-11T09:43:47.507757",
     "exception": false,
     "start_time": "2022-07-11T09:43:46.644664",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|basket                       |prediction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+-----------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[ham, yogurt, light bulbs]   |[beef, oil, detergent, chocolate, candy, berries, frankfurter, sausage, coffee, pip fruit, white bread, salty snack, domestic eggs, root vegetables, bottled beer, specialty bar, long life bakery product, rolls/buns, other vegetables, soda, whole milk, canned beer, fruit/vegetable juice, dessert, newspapers, bottled water, margarine, hamburger meat, pastry, onions, pork, chicken, herbs, soft cheese, frozen meals, frozen vegetables, UHT-milk, brown bread, citrus fruit, butter, misc. beverages, chewing gum, shopping bags, cream cheese , waffles, whipped/sour cream, butter milk, hard cheese, napkins, curd, tropical fruit]|\n",
      "|[jam, cocoa drinks, pet care]|[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+-----------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(new_df).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a87e88-f1fc-4cd3-a0e6-6cda300e2438",
   "metadata": {
    "papermill": {
     "duration": 0.172155,
     "end_time": "2022-07-11T09:43:47.852077",
     "exception": false,
     "start_time": "2022-07-11T09:43:47.679922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 109.119384,
   "end_time": "2022-07-11T09:43:50.051935",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-11T09:42:00.932551",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
